from math import logdef create_data_set():    l_data_set = [[1, 1, 'yes'], [1, 1, 'yes'], [1, 0, 'no'], [0, 1, 'no'], [0, 1, 'no']]    l_labels = ['no surfacing', 'flippers']    return l_data_set, l_labelsdef calc_shanon_entropy(p_data_set):    l_num_entries = len(p_data_set)    l_label_counts = {}    for feat_vec in p_data_set:        l_current_label = feat_vec[-1]        if l_current_label not in l_label_counts.keys():            l_label_counts[l_current_label] = 0        l_label_counts[l_current_label] += 1    l_shanon_ent = 0.0    for key in l_label_counts:        l_prob = float(l_label_counts[key])/l_num_entries        l_shanon_ent -= l_prob * log(l_prob, 2)    return l_shanon_entdef split_data_set(p_data_set, p_axis, p_value):    l_ret_data_set = []    for feat_vec in p_data_set:        if feat_vec[p_axis] == p_value:            l_reduced_feat_vec = feat_vec[:p_axis]            l_reduced_feat_vec.extend(feat_vec[p_axis+1:])            l_ret_data_set.append(l_reduced_feat_vec)    return l_ret_data_setdef choose_best_feature_to_split(p_data_set):    l_num_features = len(p_data_set[0]) - 1    l_base_entropy = calc_shanon_entropy(p_data_set)    l_best_info_gain = 0.0    l_best_feature = -1    for i in range(l_num_features):        l_feat_list = [example[i] for example in p_data_set]        l_unique_values = set(l_feat_list)        l_new_entropy = 0.0        for value in l_unique_values:            l_sub_data_set = split_data_set(p_data_set, i, value)            l_prob = len(l_sub_data_set)/float(len(p_data_set))            l_new_entropy += l_prob * calc_shanon_entropy(l_sub_data_set)        l_info_gain = l_base_entropy - l_new_entropy        if l_info_gain > l_best_info_gain:            l_best_info_gain = l_info_gain            l_best_feature = i    return l_best_feature